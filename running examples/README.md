# Overview

## Overview of the program file

In the `running examples` folder, there are 2 executable files, namely `RunController.jar` and `RunController.jar`. They are responsible for executing the data generation task in the cluster. You can compile them by `mvn package` The summary description is as follows:

1. `RunController.jar` is the manager of the cluster. It is responsible for managing the load generation tasks in each `RunDataGenenrator.jar`. It communicates through the netty network framework (sending data generation tasks and Join Information Table). Run with `java -jar RunController.jar <cluster_config>`.
2. `RunDataGenenrator.jar` can run on multiple cluster nodes and execute data generation tasks in a distributed and parallel manner. Runtime information is distributed by the RunController.jar program file in the cluster. Run with `java -jar RunDataGenenrator.jar <cluster_config> <generator_id>`. Note that `<generator_id>` is from 0.

### Quick Start
```shell
$ cd running\ examples/
$ mkdir data
$ mkdir outerJoin
$ java -jar RunController.jar touchstone.conf

#open anothor terminal
$ java -jar RunDataGenerator.jar touchstone.conf 0
```

## Overview of the configuration file

Before starting the cluster, you need to write the cluster environment configuration file and the load generation task configuration file.

+ The sample cluster configuration file is [touchstone.conf](touchstone.conf), which configures the nodes required for cluster operation, the degree of concurrency, and the running path.
+ The load generation task configuration file contains two configuration files:
  
   1. Table information (the sample is [tpch_schema_sf_1.txt](input/tpch_schema_sf_1.txt)), which describes the basic data format that the table data to be generated needs to meet, including the basic distribution of Schema information and table data.
   2. Load statement information (the sample is [tpch_cardinality_constraints_sf_1.txt](input/tpch_cardinality_constraints_sf_1.txt)), which describes the structure of the SQL statement to be tested, the filtering ratio of each intermediate result set and other characteristics.
   

In the following two chapters, we have made specific descriptions of related configuration parameters. After explaining the configuration file format, we give configuration examples of TPC-H and SSB for reference.

## Generation result

+ **Instantiated query parameters**: Generated by the Touchstone controller. The order of the parameters is the same as the order of the symbolic parameters in the input cardinality constraint.
  
+ **The generated data file**: Generated in the path configured by the data generator.



## Cluster environment configuration file

**Attention:** All cluster configuration files need to be written in one file, such as touchstone.conf in the running examples folder.

1. Configuration file of RunController.jar

   For the program file, you need to configure the ip used as the controller node in the cluster, and the port number of the node. For example, the following configuration items, configure the controller's running node as 10.11.1.190, port number as 32100. 
And then you need config the path of instantiated query parameters, which is `data/result` in the example.

   ```yaml
   ## configurations of controller
   
   IP of controller: 10.11.1.190
   port of controller: 32100
   result output directory: ./data/result
   ```

   When the controller is running, it is necessary to load the configuration files of the data generation task, including the table information of the database to be generated and the load characteristics (cardinality constraints in the constraint chain). The specific forms of these two configuration files will be described in detail below to configure the controller The configuration file input items are as follows:

   ```yaml
   ## input files
   
   database schema: ./input/tpch_schema_sf_1.txt
   cardinality constraints: ./input/tpch_cardinality_constraints_sf_1.txt
   ```

   For complex filter predicates and non-equivalent join predicates, Touchstone needs to use the numerical integration function provided by Mathematica (corresponding to the random sampling algorithm in the paper), but if the input load contains only simple filter predicates (such as col> para) and Equivalent connection predicates, no configuration is required, the way to configure Mathematica is:

   ```yaml
   ## configuration of Mathematica
   
   path of JLink: C://Mathematica//10.0//SystemFiles//Links//JLink
   ```

   The runtime logging method uses the log4j framework, which can load custom configuration files. The configuration items are:

   ```yaml
   ## configuration of log4j
   
   path of log4j.properties: ./lib/log4j.properties
   ```

2. RunDataGenerator.jar

   The program file needs to be configured with five parameters:
   1. the ip of the node used as data generator in the cluster
   2. the port number to interact with the controller
   3. the number of threads currently used by the program for data generation
   4. the folder for data output

   A set of configuration examples are given below. The data generator program is configured on two machines, 191 and 192. Each machine runs 3 instances, and each instance runs 2 generation threads. The port number on each machine is 32101. ; 32102; 32103, the output folder is .//data.

   ```yaml
   ## configurations of data generators
   
   IPs of data generators: 10.11.1.191; 10.11.1.191; 10.11.1.191; 10.11.1.192; 10.11.1.192; 10.11.1.192
   ports of data generators: 32101; 32102; 32103; 32101; 32102; 32103
   thread numbers of data generators: 2; 2; 2; 2; 2; 2
   data output path: ./data
   ```

   Since the performance of starting multiple data generation threads in one JVM is often not as good as starting the same number of data generation threads in multiple JVMs, it is recommended to start multiple JVMs on a node according to the number of physical CPU cores. The above example configuration starts 3 JVMs on each physical node, and 2 data generation threads are started in each JVM. All operating directories will be created automatically, without manual creation.

3. Some parameters required during the operation of Touchstone

   This part of the parameters generally does not need to be changed, and the default values can be used directly. For details, please refer to the paper.

   ```yaml
   ## running parameters
   
   # The controller is calculating the control parameters of query instantiation
   thread numbers of query instantiation: 2  #Number of threads used for calculation
   maximum iterations of query instantiation: 20 #Number of iterations
   global relative error of query instantiation: 0.0001 #Global approximation error
   maximum iterations of parameter instantiation: 20 #Maximum number of iterations of the parameter
   relative error of parameter instantiation: 0.0001 #Approximation error of a single parameter
   
   
   # The maximum number of shuffles used to calculate the constraint rules, see Algorithm 3 in the technical report
   maximum number of shuffle: 1000 
   
   #The maximum number of primary keys for each status in Jointable, that is, L in the compression algorithm
   maximum size of PKVs: 10000 
   ```
4. Some parameters can be used to configure the left outer join and are generally not recommended being modified.

    ```yaml
    ## outer join
    join info output path: ./outerJoin
    maximum num of join table file in read or write queue: 5
    maximum size of join table in memory to write : 200
    maximum size of join table in memory to read : 200
    minimum num of join table status to read: 20
    ```


## Task profile for workload generation

Touchstone has two input data files, which respectively contain database schema information (including data feature information) and cardinality constraint information.


### Table information of the database

1. Schema information

   ```yaml
   ## Input format
   
   T[table_name; table_size; column_name, data_type; ...; P(primary_key); F(foreign_key, referenced_table.referenced_primary_key); ...]  
   
   ## Input sample, take TPC-H table PARTSUPP as an example
   
   T[PARTSUPP; 8000000; PS_PARTKEY, INTEGER; PS_SUPPKEY, INTEGER; PS_AVAILQTY, INTEGER; PS_SUPPLYCOST, DECIMAL; PS_COMMENT, VARCHAR; P(PS_PARTKEY, PS_SUPPKEY); F(PS_PARTKEY, PART.P_PARTKEY); F(PS_SUPPKEY, SUPPLIER.S_SUPPKEY)]
   ```

2. Basic data characteristics

   ```yaml
   # Integer:
   D[table_name.column_name; null_ratio; cardinality; min_value; max_value]  
   # Example:
   D[T1.c1; 0.05; 100; 1; 9999]
   
   # Real & Decimal:
   D[table_name.column_name; null_ratio; min_value; max_value]  
   # Example: 
   D[T1.c2; 0; 0.1; 1000.5]
   
   # Varchar: 
   D[table_name.column_name; null_ratio; avg_length; max_length]  
   # Example:  
   D[T1.c3; 0; 123.5; 199]
   
   # Bool:  
   D[table_name.column_name; null_ratio; true_ratio]  
   # Example: 
   D[T1.c4; 0.2; 0.6]
   
   # DateTime:  
   D[table_name.column_name; null_ratio; begin_time; end_time]  
   # Example:  
   D[T1.c5; 0; 1992-01-02-00:00:00; 1998-12-01-00:00:00]
   
   # Date:  
   D[table_name.column_name; null_ratio; begin_time; end_time]  
   # Example:  
   D[T1.c6; 0; 1992-01-02; 1998-12-01]
   ```

### Workload characteristics (cardinality constraints in the constraint chain)

**There are three types of cardinal constraints in the constraint chain, and the basic data structure is as follows:**  

1. Filter node:

   ```yaml
   Filter node: [0, exp1@op1#exp2@op2 ... #and|or, probability]
   ```

   **Description:**
   
   + "0" indicates that this is a selection node;
   + "exp1@op1#exp2@op2 ... #and|or" describes all selection conditions in the current selection operation;
   + "probability" is the filtering rate.

2. Primary key node (the attribute of the current data table in the equivalent join operation is the primary key attribute)
   ```yaml
   PKJoin node: [1, pk1#pk2 ..., num1, num2, ...] 
   ```
   **Description:**
   
   + "1" indicates that this is a primary key node;
   + "pk1#pk2..." is the primary key (if the primary key is a composite primary key, this is a property collection);
+ "num1" & "num2" are the codes when maintaining primary key connection information.
   
3. Foreign key node (the attribute of the current data table in the equivalent join operation is a foreign key attribute)
   ```yaml
   FKJoin node: [2, fk1#fk2 ..., probability, pk1#pk2 ..., num1, num2]
   ```
   **Description:**
   
   + "2" indicates that this is a foreign key node;
   + "fk1#fk2..." is a collection of foreign key attributes;
   + "probability" is the joining rate;
   + "pk1#pk2..." is the primary key of the reference (if the reference is a composite primary key, this is a property set);
   + "num1" & "num2" are the codes of the corresponding primary key connection information.

The following will introduce the specific format of the input data based on some sample Query input of TPC-H.

1. Physical query tree of TPC-H's Query 1 on MySQL
   
   <img src="http://ww3.sinaimg.cn/large/006tNc79ly1g3zap9kpbzj30d408naal.jpg" width="240" height="155" />

The corresponding constraint chain is:

```
[lineitem]; [0, l_shipdate@<=, 0.985899]
```

**Description：**

+ A constraint chain is for a specific data table, where [lineitem] indicates that the constraint chain is for the data table lineitem.
+ [0, l\_shipdate@<=, 0.985899]
  + "0" indicates that this is a Filter constraint node;
  + "l\_shipdate@<="Used to describe the filtering predicate"l\_shipdate <= p1"；
  + "0.985899" is the selection rate (=5916591/6001215).

2. Physical query tree of TPC-H's Query 3 on MySQL

   ![TPC-H_Query-3](http://ww2.sinaimg.cn/large/006tNc79ly1g3zaqvbz1pj30d40cv0t3.jpg)

The corresponding constraint chain is:

```
[customer]; [0, c_mktsegment@=, 0.20095]; [1, c_custkey, 1, 2] 
[orders]; [2, o_custkey, 0.20264, customer.c_custkey, 1, 2]; [0, o_orderdate@<, 0.48403]; [1, o_orderkey, 1, 2] 
[lineitem]; [2, l_orderkey, 0.09806464, orders.o_orderkey, 1, 2]; [0, l_shipdate@>, 0.05185835]
```

**Description：**

Since Query 3 involves three data tables, there are three constraint chains.

"[1, c\_custkey, 1, 2]"in the first constraint chain, the first "1" indicates that this is a PK constraint node (the attribute of the customer table in this equivalent join operation is the primary key attribute ), the following "1, 2" is the code used to maintain the primary key connection information.

"[2, o\_custkey, 0.20264, customer.c\_custkey, 1, 2] "in the second constraint chain, the first "2" indicates that this is an FK constraint node (the orders table is equivalent in this The attributes in the join operation are foreign key attributes). "o\_custkey" is the foreign key attribute name, "0.20264" is the connection rate (=303959/1500000), "customer.c\_custkey" is the reference primary key, and "1, 2" is the connection operation phase of the FK constraint node The code in the corresponding PK constraint node (ie "[1, c\_custkey, 1, 2]").

The probability in all cardinality constraints is either the selection rate or the connection rate, which is calculated based on the size of the intermediate result set in the actual query tree.

>  **Explanation on encoding:**
>
> When a tuple in the customer table satisfies the "c\_mktsegment = p7" selection operation, then the primary key c\_custkey of this tuple may be connected to the next orders table. For such c\_custkey, we will label it "1"; For those c\_custkey that does not meet the "c\_mktsegment = p7" selection operation, label "2" to indicate that they must not be connected to the following orders table. The codes are all 2^n, which are currently guaranteed by input. In fact, these codes can be automatically generated by the program without manual input, and additional tools will be provided later.
>
> **Why is the code 2^n？**
>
> Because it is necessary to use the sum of the codes of the primary key of the current tuple in all constraint chains to represent its connection state (equivalent to n bits, each two bits correspond to a connection operation, and only one of these two bits can be 1 To indicate whether this primary key can be connected)!

## How to convert the SQL statement into the input of the constraint chain configuration file

In this section, we will take Query3 of TPC-H as an example to explain in detail how to convert Query into a constraint chain from an existing data set.

+ First, use the tool to generate the TPC-H sample library with SF=1 on MySQL, and then initialize the Query3 statement as:

  ```sql
  select
          l_orderkey,
          sum(l_extendedprice * (1 - l_discount)) as revenue,
          o_orderdate,
          o_shippriority
  from
          customer,
          orders,
          lineitem
  where
          c_mktsegment = 'BUILDING'
          and c_custkey = o_custkey
          and l_orderkey = o_orderkey
          and o_orderdate < date '1995-03-15'
          and l_shipdate > date '1995-03-15'
  group by
          l_orderkey,
          o_orderdate,
          o_shippriority
  order by
          revenue desc,
          o_orderdate;
  limit 10;
  ```

+ Use explain+query to display the MySQL query plan, you can see that the Join order is cutomer->orders->lineitem.

  | id | select_type | table    | partitions | type | possible_keys      | key        | key_len | ref                     | rows   | filtered | Extra|
  | ---- | ------ | -------- | ---- | ---- | ------- | ---- | ---- | ---- | ---- | ---- | ---- |
  |  1 | SIMPLE      | customer | NULL       | ALL  | PRIMARY            | NULL       | NULL    | NULL                    | 147408 |    10.00 | Using where; Using temporary; Using filesort |
  |  1 | SIMPLE      | orders   | NULL       | ref  | PRIMARY,ORDERS_FK1 | ORDERS_FK1 | 4       | tpch.customer.C_CUSTKEY |     15 |    33.33 | Using where       |
  |  1 | SIMPLE      | lineitem | NULL       | ref  | PRIMARY            | PRIMARY    | 4       | tpch.orders.O_ORDERKEY  |      4 |    33.33 | Using where                                  |

+ Use the order of the query plan to execute the sub-statements step by step to know the size of the data set at each step

  ```mysql
  select count(*) from customer;#150000
  
  select count(*) from customer where c_mktsegment = 'BUILDING';#30142
  
  select count(*) from orders;#1500000
  
  select count(*) from customer,orders 
  where c_mktsegment = 'BUILDING' and c_custkey = o_custkey;#303959
  
  select count(*) from customer,orders 
  where c_mktsegment = 'BUILDING' and c_custkey = o_custkey
  and ;o_orderdate < date '1995-03-15'#147126
  
  select count(*) from lineitem;#6001215
  
  select count(*) from customer, orders, lineitem
  where c_mktsegment = 'BUILDING' and c_custkey = o_custkey
  and l_orderkey = o_orderkey and o_orderdate < date '1995-03-15'#588507
  
  select count(*) from customer, orders, lineitem
  where c_mktsegment = 'BUILDING' and c_custkey = o_custkey
  and l_orderkey = o_orderkey and o_orderdate < date '1995-03-15'
  and l_shipdate > date '1995-03-15'#30519
  ```

+ Combined with the size of the data set, the query plan can be found in the following query tree, and the configuration file of the constraint chain can be generated through the query tree.

![TPC-H_Query-3](http://ww2.sinaimg.cn/large/006tNc79ly1g3zaqvbz1pj30d40cv0t3.jpg)

## Standard sample of cluster configuration file

Please see the standard configuration file of the system running program: touchstone.conf

+ TPC-H standard configuration file when sf=1:

  Schema configuration file: [tpch_schema_sf_1](input/tpch_schema_sf_1.txt)

  Constraint configuration file for the first 16 statements: [tpch_cardinality_constraints_sf_1.txt](input/tpch_cardinality_constraints_sf_1.txt)

  Example diagram of constraint configuration for the first 16 sentences: TPC-H Query1-16 SF=1.png

+ ssb standard configuration file when sf=1:

  Schema configuration file: [ssb_schema_sf_1](input/ssb_schema_sf_1.txt)

  Constraint configuration file: [ssb_cardinality_constraints_Q1-Q4.txt](input/ssb_cardinality_constraints_Q1-Q4.txt)
